# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCfUQSHcxbPIdA5j_r5RSOfXh2Fx46Ue
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os

# -----------------------------
# 1. Generate mock transaction data
# -----------------------------
np.random.seed(42)
num_transactions = 5000
num_customers = 500
products = [f'Prod_{100+i}' for i in range(20)]
start_date = datetime(2022, 1, 1)
end_date = datetime(2023, 12, 31)
date_range_days = (end_date - start_date).days

transaction_data = []

# Customer spending profiles
customer_profiles = {
    f'CUST{1000+i}': {
        'avg_spend': np.random.uniform(10, 200),
        'frequency_factor': np.random.uniform(0.1, 1)
    }
    for i in range(num_customers)
}

for i in range(num_transactions):
    customer_id = f'CUST{1000 + np.random.randint(0, num_customers)}'
    transaction_date = start_date + timedelta(days=np.random.randint(0, date_range_days))
    num_items_in_transaction = np.random.randint(1, 6)

    for j in range(num_items_in_transaction):
        product_id = np.random.choice(products)
        quantity = np.random.randint(1, 4)
        unit_price = np.random.uniform(5, 100) * (customer_profiles[customer_id]['avg_spend'] / 50)
        unit_price = round(max(1.0, unit_price), 2)

        transaction_data.append({
            'TransactionID': f'TRX{50000+i}_{j}',
            'CustomerID': customer_id,
            'TransactionDate': transaction_date.strftime('%Y-%m-%d'),
            'ProductID': product_id,
            'Quantity': quantity,
            'UnitPrice': unit_price
        })

df_transactions = pd.DataFrame(transaction_data)
df_transactions['TotalPrice'] = df_transactions['Quantity'] * df_transactions['UnitPrice']

# -----------------------------
# 2. Save to CSV
# -----------------------------
csv_file = "customer_transactions_mock_data.csv"
df_transactions.to_csv(csv_file, index=False)

print(f"‚úÖ Mock customer transaction data generated: {csv_file}")
# -----------------------------
# Data Loading and Preparation
# -----------------------------

# -----------------------------
# Data Loading and Preparation
# -----------------------------

# Load the generated dataset
df = pd.read_csv("customer_transactions_mock_data.csv")

# Convert TransactionDate column to datetime
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')

# Handle missing values (drop rows with null values)
df = df.dropna()

# Remove outliers: keep only positive Quantity and UnitPrice
df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]

print("\n‚úÖ Cleaned Data Preview:")
from IPython.display import display
display(df.head())


print(f"üìÇ File saved in: {os.getcwd()}")


# -----------------------------
# 3. Show ALL rows and columns
# -----------------------------
#pd.set_option('display.max_rows', None)
#pd.set_option('display.max_columns', None)#
#pd.set_option('display.width', None)
#pd.set_option('display.colheader_justify', 'left')

#print("\nFull Dataset Preview:")
#print(df_transactions)

# -----------------------------
# 4. RFM Calculation
# -----------------------------
snapshot_date = pd.to_datetime(end_date + timedelta(days=1))
df_transactions['TransactionDate'] = pd.to_datetime(df_transactions['TransactionDate'])

rfm = df_transactions.groupby('CustomerID').agg(
    Recency=('TransactionDate', lambda x: (snapshot_date - x.max()).days),
    Frequency=('TransactionID', 'nunique'),   # unique transactions per customer
    MonetaryValue=('TotalPrice', 'sum')
).reset_index()

print("\nüìä Sample RFM data (first 5 rows):")
print(rfm.head())

# -----------------------------
# 5. (Optional) Download CSV if using Colab
# -----------------------------
try:
    from google.colab import files
    files.download(csv_file)
except:
    print("\n(‚ÑπÔ∏è If you‚Äôre on Colab, uncomment the files.download line above to download the CSV)")
    from IPython.display import display  # üëà Import once at the top

# -----------------------------
# Show ALL rows and columns settings
# -----------------------------
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.colheader_justify', 'left')

print("\nüìä Full Dataset Preview:")
display(df_transactions)   # üëà Instead of print(df_transactions)

print("\nüìä Sample RFM data (first 5 rows):")
display(rfm.head())
df = df.dropna()
df.dropna(subset=['TransactionDate'])
# -----------------------------
# 4. RFM Calculation
# -----------------------------
snapshot_date = df['TransactionDate'].max() + pd.Timedelta(days=1)

rfm = df.groupby('CustomerID').agg(
    Recency=('TransactionDate', lambda x: (snapshot_date - x.max()).days),
    Frequency=('TransactionID', 'nunique'),
    Monetary=('TotalPrice', 'sum')
).reset_index()

print("\nüìä RFM Metrics (first 5 rows):")
print(rfm.head())
import matplotlib.pyplot as plt

# Histogram plots
rfm[['Recency', 'Frequency', 'Monetary']].hist(bins=30, figsize=(12,6))
plt.show()

# Skewness check
print("Skewness:")
print(rfm[['Recency', 'Frequency', 'Monetary']].skew())
import numpy as np

# Avoid log(0) ‚Üí add 1
rfm_log = rfm.copy()
rfm_log['Recency'] = np.log1p(rfm['Recency'])
rfm_log['Frequency'] = np.log1p(rfm['Frequency'])
rfm_log['Monetary'] = np.log1p(rfm['Monetary'])

# Plot again to check distributions
rfm_log[['Recency', 'Frequency', 'Monetary']].hist(bins=30, figsize=(12,6))
plt.show()
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Option 1: StandardScaler (mean=0, std=1)
scaler = StandardScaler()

# Option 2: MinMaxScaler (values between 0 and 1)
# scaler = MinMaxScaler()

rfm_scaled = scaler.fit_transform(rfm_log[['Recency', 'Frequency', 'Monetary']])

# Convert back to DataFrame for convenience
rfm_scaled = pd.DataFrame(rfm_scaled, columns=['Recency', 'Frequency', 'Monetary'], index=rfm['CustomerID'])

print("\n‚úÖ Scaled RFM data preview:")
print(rfm_scaled.head())
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Find optimal k with Elbow
wcss = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(rfm_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(range(2, 11), wcss, marker='o')
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.title("Elbow Method")
plt.show()

# Optional: silhouette score
for k in range(2, 11):
    labels = KMeans(n_clusters=k, random_state=42, n_init=10).fit_predict(rfm_scaled)
    print(f"k={k}, silhouette={silhouette_score(rfm_scaled, labels):.3f}")
# Apply final model with chosen k
best_k = 4  # assume from elbow/silhouette
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)
cluster_summary = rfm.groupby('Cluster').agg({
    'Recency':'mean',
    'Frequency':'mean',
    'Monetary':'mean',
    'CustomerID':'count'
}).rename(columns={'CustomerID':'NumCustomers'})

print(cluster_summary)
from sklearn.decomposition import PCA

pca = PCA(2)
rfm_pca = pca.fit_transform(rfm_scaled)

plt.figure(figsize=(8,6))
plt.scatter(rfm_pca[:,0], rfm_pca[:,1], c=rfm['Cluster'], cmap='viridis')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Customer Segments')
plt.colorbar(label='Cluster')
plt.show()